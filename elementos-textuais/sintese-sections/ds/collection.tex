\subsection{Coleta de Dados}
A coleta, é o ato de obter dados em massa de fontes externas, como \textit{websites} e \textit{apis}\footnote{A sigla API vem de \textit{Application Programming Interface} e é uma interface que permite outras aplicações utilizarem de seus recursos e funcionalidades. Dentro do universo \textit{web}, o termo API é utilizado para descrever um conjunto de rotas que podem ser utilizados para acessar recursos de um aplicação online.}. Também conhecido como \textit{data collection}, durante o processo do KDD é o primeiro passo responsável por gerar um amostra mais focada no problema.

No projeto será utilizada a API do Twitter para coletadar os dados para uso da aplicação. Durante a coleta é necessário entender as limitações da fonte de dados. No caso das API's é usual existir \textit{Rate Limits}, ou seja, limites de requisições, no caso do Twitter existem planos que aumentam ou diminuem seu acesso a API, no caso do projeto será utilizado a versão \textit{Standard} que é a versão gratuita para desenvolvedores\footnote{\url{https://developer.twitter.com/en/docs/basics/rate-limits.html}}. Alguns dos \textit{endpoints}\footnote{Um endpoint é uma URL acessivel por protocolos HTTP ou HTTPS e que retornam dados ou realizam ações} que serão:
\begin{itemize}
    \item \textit{users/show}: Responsável por pegar as informações do usuário, tem um limite de 60 requisições por minuto.
    \item \textit{statuses/user\_timeline}: Responsável por pegar os últimos tweets do usuário, tem um limite de 100 requisições por minuto.
\end{itemize}

Existe no projeto também, e recorrente em outros tipos de APIs, os \textit{streamings}, ou seja, recursos que serão consumidos através de uma requisição que fica aberta por tempo indeterminado recebendo novos dados.
\begin{itemize}
    \item \textit{statuses/filter}: Responsável por pegar os últimos tweets publicados por filtros. Na documentação será detalhado os filtros que serão utilizados na pesquisa, mas basicamente aqui é possível escutar as ultimas publicações feitas em escala global e coletar apenas as que compatíveis com um conjunto de filtros. Esse tipo de recurso não tem seu limite estabelecido por requisições e sim pelo seu uso (tentativas de reconexões por exemplo).
\end{itemize}

Uma vez coletados, outras ações descritas na Figura \ref{fig:kdd} como Pré-Processamento e Mutação serão realizados no próprio coletor enquanto outras serão realizadas posteriormente por \textit{scripts} e/ou ferramentas. A preparação do dado é relevante para assegurar a qualidade da mineração. No Twitter pode-se encontrar diversos problemas na hora da análise. Alguns dos mais populares são: A quantidade de ruído apresentado nos dados por erros de ortográfica, os uso de símbolos especiais como \textit{emojis} ou a inserção \textit{urls} ao longo do texto e por final a informalidade e contexto multilíngue, ou seja, o ato de escrever-se utilizando palavras fora do nosso idioma ou gírias pra manifestar algo. Além disso existe o fato de alguns processamentos, como a análise de sentimento, ter alguns problemas em textos muito curtos ou muito compridos. Pesquisas de 2012\footnote{\url{https://thenextweb.com/twitter/2012/01/07/interesting-fact-most-tweets-posted-are-approximately-30-characters-long/}}, demonstram que um tweet contém em média apenas 30 caracteres com o seu limite máximo, já pequeno, de 140 carácteres. \cite[9-11]{silva2016analise} Entretanto, no dia 27 de setembro de 2017, o Twitter anunciou\footnote{\url{https://blog.twitter.com/official/en_us/topics/product/2017/Giving-you-more-characters-to-express-yourself.html}} a expansão do limite máximo para 280 caracteres, o que abre possibilidade de textos mais pontuais na massa de dados coletada.

Uma vez conhecida as limitações e problemas da fonte de coleta, é possível idealizar e implementar soluções para contornar e/ou utilizar alguns recursos apresentados problemáticos.
