\section{Coletando Base de Dados}
A primeira etapa do processo inclui coletar dos dados para, que posteriormente, seja possível mineirar dados dentro dele. O coletor é algo simples, basicamente existem 3 pontos de acesso da api, como já dito previamente na revisão, que serão consultados. Foi desenvolvida uma função responsável por coletar em tempo real tweets na área do Brasil que em seu corpo tivesse uma ou mais palavras chaves, feito isso, foi coletado os dados do usuário que fez aquele tweet e os últimos 200 tweets do mesmo.

De todos os modelos previamente explicados, os utilizados aqui são os de \textit{user} e \textit{tweet}. Eles podem ser encontrados dentro de \textit{dumont/coletor/schemas}. Antes do dado ser salvo é possível notar a criação de um objeto partindo dos atributos de texto referentes ao tweet, isso se deve a um dos problemas durante a mineração de dados ser o uso de \textit{emojis} em textos. Sabendo que \textit{emojis} podem expressar sentimentos, e que armazenar e tratar esse dado poderia ser relevante na hora de confirmar sentimento em frases, o autor também criou a biblioteca \textit{Emojinator}\footnote{https://github.com/getdumont/emojinator}, além do texto tradado, também será obtida informações do \textit{emojis} utilizados no meio do texto.

Essa função foi posta para ser executada dentro de um CLI\footnote{CLI é abreviatura para \textit{Command Line Interface}, em outras palavras, uma interface que permite executar códigos direto do terminal}. Para executar o commando de coleta é necessário primeiramente é necessário configurar o projeto como descrito na página \pageref{app:configuracoes}. Uma vez configurado, basta rodar o comando \textit{docker-compose up collector}, esse processo vai preencher seu MongoDB com os dados necessários para as próximas etapas. Durante essa pesquisa o comando foi rodado diversas vezes em um periodo de tempo, para criar uma base inicial.

No primeiro momento que o coletor foi rodado na pesquisa, foram coletados um total de 68583 tweets distribuídos entre 419 perfis, gerando uma média de aproximadamente 163 tweets por usuário. A massa de dados é bem ampla e achar uma amostra que suprisse as necessidades poderia ser algo complexo. Para isso o enfoque é localizar uma amostra com os perfis que contem a maior massa de tweets negativos, entretanto, isso não é possível sem idealizar uma segunda parte do processo, no caso o pré-processamento.
