\subsection{Tarefas de Pré-Processamento}
O segundo passo após coletar os dados é rodar scripts de pré-processamento e mineração. Uma das maiores dificuldades é como manipular os dados de maneira incremental. Desde que a pesquisa teve inicio, muitas ideias surgiram, novos pontos de vistas e novos dados a serem minerados. Foi adotado uma propriedade chamada \textit{processing\_version}, essa propriedade marca o documento com a versão do processamento dele.

Dentro da pasta \textit{dumont/tasks/processing} existem duas pastas, com código para processar usuários e outra para processar tweets, ambas contem vários estágios de processamento, esses são exportados e passados para uma classe chamada \textit{Processor} localizada no arquivo \textit{dumont/tasks/processing/\_\_init\_\_.py}, o nível de processamento base é o 0 (nível inserido na hora que o coletor salva do dado no banco), a partir disso é possível atualizar o processamento por um script.

As tarefas de processamento são responsáveis por:

\begin{itemize}
    \item Normalização de Palavras: Transformar girias e erros conhecidos em palavras corretas.
    \item Remoção de \textit{stop-words}: Existem palavras que prejudicam a analise textual por não serem essenciais ou estarem colocadas de maneira equivocada. Um dos processos retira esse tipo de ruído do texto.
    \item Arvore Léxica: Criar uma arvore léxica baseada na frase original do tweet e na frase que já foi tratada removendo as \textit{stop-words}.
    \item Analise de Sentimento: Utilizando a API do Google Language é retirado o sentimento da frase original e da frase tratada também.
\end{itemize}

As tarefas sofreram aprimoramento durante a analise de dados, entretanto, o que será exposto aqui é o estagio de refinamento obtido até o momento junto com seus resultados. Enquanto a análise era feita foi notado uma quantidade de girias e expressões que podiam influenciar a analise de sentimento, tão quanto a extração de arvore lexica. Fazendo assim com que dois dos principais atributos a serem mapeados ficassem inconsistentes. Conforme \ref{app:tabelanorm} que traz uma tabela com termos, é possivel notar todas as palavras, as substituições e a quantidade encontrada no banco inicial.

Além disso, foi encontrado um outro padrão devido a uma recente moda chamada CuriousCat\footnote{\url{https://curiouscat.me}}, um site de perguntas e respostas. Muitos tweets contem textos referentes a função de compartilhamento da plataforma. Algumas respostas podem ajudar na inferencia, logo foi criado um script para substituir o tweet, usualmente formado pela pergunta juntamente com uma parte da resposta, por unicamente a resposta completa(indiferente do seu tamanho), para isso foi criado um \textit{WebScraping}\footnote{termo utilizado para \textit{scripts} que "raspam" ou coletão dados de páginas da web através de API ou de algoritimos de automação que buscam valores em telas HTML} que pode ser encontra no arquivo \textit{dumont/tasks/processing/entity\_parser/url/curiouscat.py}\footnote{Código fonte no GitHub: \url{}}.

Um outro grande problema apresentado durante a analise é que muitos tweets expressavam sentimentos, porem seu discurso era dividido, ou seja, o texto em si representava apenas uma parte do conteúdo da publicação. No caso existem tweets unicamente com imagem e casos onde o texto é uma mera legenda para a imagem, além disso existem publicações com vídeos e publicações divididas em vários tweets, ou seja, expressões como \textit{emojis}, risadas e outras ferramentas dissertativas que não trazem diretamente um conteudo e sim um apoio a um ou mais tweets anterioriores. Entretanto tratar isso levaria tempo, então nesse primeiro passo da pesquisa esses dados foram ignorados.

Todas as alterações geram novos atributos, principalmente o \textit{clean\_text} que servira como base para retirada de informações, para executar o \textit{script} necessário para essa ação basta rodar o comando \textit{docker-compose up tasks}. Com esses dados já é possível ter alguma noção de informações relevantes dos textos, porém ainda é necessário de embasamento técnico, ou seja, um dado especialista que possa orientar a máquina a utilizar as demais propriedades mapeadas para localizar um delta em comum, entretanto, exigir que todos os dados da massa sejam analisados é algo inviavel. Para isso é necessário gerar uma pré-amostra.