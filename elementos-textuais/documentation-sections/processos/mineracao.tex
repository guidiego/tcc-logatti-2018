\subsection{Mineração}
O segundo passo após coletar os dados existem scripts de mineração. Uma das maiores dificuldades é como manipular os dados de maneira incremental. Desde que a pesquisa teve inicio, muitas ideias surgiram, novos pontos de vistas e novos dados a serem minerados. Adotamos uma propriedade chamada \textit{processing_version}, essa propriedade marca o documento com a versão do processamento dele.

Dentro da pasta \textit{dumont/tasks/processing} existe duas pastas, uma para processar usuarios e outra para processar tweets, amabas esportão varios estagios de processamento, esse estágio é exportado e passado para uma classe chamada \textit{Processor} localizada no arquivo \textit{dumont/tasks/processing/__init__.py}, o level de processamento base é o 0 (level inserido na hora que o coletor salva do dado no banco), a partir disso é possível atualizar o processamento por um script (no caso existe a possibilidade de consumo da fila, porem, como ja dito essa abordagem esta contida no anexo 1).

Quando o coletor foi configurado, automaticamente todos os dados necessários para o processamento ja foram preenchidos. A tarefa de processamento é responsavel por:

\begin{itemize}
    \item Remoção de \textit{stop-words}: Existem palavras que prejudicam a analise textual por não serem essenciais ou estarem colocadas de maneira equivocada. Um dos processos retira esse tipo de ruido do texto.
    \item Arvore Léxica: Criar uma arvore lexica baseada na frase original do tweet e na frase que ja foi tratada removendo as \textit{stop-words}.
    \item Analise de Sentimento: Utilizando a API do Google Language é retirado o sentimento da frase original e da frase tratada tambem.
\end{itemize}

Para obter esses dados basta rodar o comando \textit{docker-compose -f docker-compose.dev.yml up tasks}. Com esses dados já é possivel ter alguma noção de informações relevantes dos textos, porém ainda é necessário de embasamento técnico, ou seja, um dado especialista que possa orientar nossa máquina a utilizar as demais propriedades mapeadas para localizar um delta em comum.

Existe um outro serviço contido dentro de \textit{dumont/specialist-api} e \textit{dumont/specialist-app} que gera uma API e uma interface gráfica para injeção de dados especialistas. Para subir ambos os serviços basta utilizar o comando \textit{docker-compose -f docker-compose.dev.yml up specialist-app}. Entretando, é necessário de um usuario para injetar as analises, é possível criar o documento na mão dentro do mongo, porem, existe um binario dentro da pasta chamado \textit{dumont/create\_specialist}, basta executa-lo passando o e-mail e ele te devolvera uma senha aleatória. Então basta acessar \textit{\url{http://127.0.0.1:3000/}} e utilizar os dados para acessar o sistema. Logo que autenticado você encontrara uma tela igual a da figura \ref{}, nela existe o tweet, uma area para adicionar perguntas da EADS relacionadas, e um local onde pode-se identificar palavras chaves dentro daquele tweet.

Com o sistema rodando, e dados sendo coletados e analisados é necessário uma amostragem para melhor acertividade e desenvolvimento. Durante a pesquisa, em uma preliminar foram coletados mais de 160GB de dados. A amostra foi tirada antes mesmo da inserção de dados especialistas, logo é necessário conhecer os \textit{scripts} de seleção do dumont.